import os
import time
import json
import re
import traceback
from contextlib import contextmanager
from datetime import datetime
from typing import Optional, Dict

import requests
from bs4 import BeautifulSoup
import mysql.connector
from mysql.connector import Error as MySQLError

from openai import OpenAI

from dotenv import load_dotenv
load_dotenv()



# Playwright
try:
    from playwright.sync_api import sync_playwright
    _PLAYWRIGHT_AVAILABLE = True
except Exception:
    _PLAYWRIGHT_AVAILABLE = False


# =========================
# 0) í™˜ê²½ì„¤ì •
# =========================
BASE_DIR = os.path.abspath(os.getcwd())
OUT_DIR = os.path.join(BASE_DIR, "notices_pdf")
os.makedirs(OUT_DIR, exist_ok=True)

# DB ì—°ê²° ì •ë³´
DB_CONFIG = {
    "host": "uoscholar.cdkke4m4o6zb.ap-northeast-2.rds.amazonaws.com",
    "user": "admin",
    "password": "dongha1005!",
    "database": "uoscholar_db",
    "port": 3306,
    "charset": "utf8mb4",
    "autocommit": False,
    "use_pure": True,
    "connection_timeout": 10,
    "raise_on_warnings": True,
}

# OpenAI
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=OPENAI_API_KEY)
SUMMARIZE_MODEL = "gpt-4o-mini"
EMBED_MODEL = "text-embedding-3-small"

# ì¹´í…Œê³ ë¦¬ â†” list_id ë§¤í•‘
CATEGORIES: Dict[str, str] = {
    "COLLEGE_ENGINEERING": "20013DA1",
    "COLLEGE_HUMANITIES": "humna01",
    "COLLEGE_SOCIAL_SCIENCES": "econo01",
    "COLLEGE_URBAN_SCIENCE": "urbansciences01",
    "COLLEGE_ARTS_SPORTS": "artandsport01",
    "COLLEGE_BUSINESS": "20008N2",
    "COLLEGE_NATURAL_SCIENCES": "scien01",
    "COLLEGE_LIBERAL_CONVERGENCE": "clacds01",
    "GENERAL": "general",     # TODO
    "ACADEMIC": "academic",   # TODO
}

BASE_URL = "https://www.uos.ac.kr/korNotice/view.do"
REQUEST_SLEEP = 1.0
MISSING_BREAK = 3
PLAYWRIGHT_TIMEOUT_MS = 45000  # í˜ì´ì§€ ë¡œë”©/ë„¤íŠ¸ì›Œí¬ ì•ˆì •í™” ëŒ€ê¸°

SUMMARY_PROMPT = (
    "ì´ ë¬¸ì„œëŠ” ëŒ€í•™ ê³µì§€ì‚¬í•­ì…ë‹ˆë‹¤. ì‚¬ëŒì´ ë°”ë¡œ í™œìš©í•  ìˆ˜ ìˆê²Œ í•µì‹¬ë§Œ ê°„ê²°íˆ ì •ë¦¬í•´ ì£¼ì„¸ìš”.\n"
    "- ì œëª©, ì£¼ìµœ/ë¶€ì„œ, ê¸°ê°„(ëª¨ì§‘/í–‰ì‚¬), ì¥ì†Œ, ëŒ€ìƒ/ìê²©, ë¬¸ì˜(ì „í™”/ë©”ì¼)ì™€ ê´€ë ¨í•˜ì—¬, ë¬¸ì„œì˜ ì „ì²´ ë‚´ìš©ì„ í•œ ë‹¨ë½ìœ¼ë¡œ ì •ë¦¬\n"
    "- ë‚ ì§œëŠ” YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”, ìˆ˜ì¹˜ëŠ” ê·¸ëŒ€ë¡œ ë³´ì¡´, ë¶ˆí•„ìš”í•œ ì•ˆë‚´/ë„¤ë¹„ê²Œì´ì…˜ ë¬¸êµ¬ëŠ” ì œì™¸\n"
)


# =========================
# 1) ìœ í‹¸
# =========================
@contextmanager
def mysql_conn():
    conn = mysql.connector.connect(**DB_CONFIG)
    try:
        yield conn
        conn.commit()
    except Exception:
        conn.rollback()
        raise
    finally:
        conn.close()


def parse_date_yyyy_mm_dd(text: str) -> Optional[str]:
    m = re.search(r"(\d{4}-\d{2}-\d{2})", text or "")
    return m.group(1) if m else None


# =========================
# 2) Playwrightë¡œ PDF ìƒì„± (ì„±ê³µ ì‹œì—ë§Œ ë‹¤ìŒ ë‹¨ê³„ ì§„í–‰)
# =========================
def render_pdf_playwright(url: str, out_pdf: str, timeout_ms: int = PLAYWRIGHT_TIMEOUT_MS) -> bool:
    if not _PLAYWRIGHT_AVAILABLE:
        print("âŒ PDF ìƒì„± ì‹¤íŒ¨: Playwrightê°€ ì„¤ì¹˜/ì„í¬íŠ¸ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return False
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True, args=["--disable-web-security"])
            page = browser.new_page()
            # ë¡œë”© â†’ networkidleë¡œ ì•ˆì •í™” â†’ ì¶”ê°€ 2ì´ˆ
            page.goto(url, wait_until="domcontentloaded", timeout=timeout_ms)
            page.wait_for_load_state("networkidle", timeout=timeout_ms)
            page.wait_for_timeout(2000)
            pdf_bytes = page.pdf(format="A4", print_background=True)
            with open(out_pdf, "wb") as f:
                f.write(pdf_bytes)
            browser.close()
        print(f"ğŸ“„ PDF ìƒì„± ì„±ê³µ: {out_pdf}")
        return True
    except Exception as e:
        print(f"âŒ PDF ìƒì„± ì‹¤íŒ¨: {e.__class__.__name__}: {e}")
        tb = traceback.format_exc(limit=3)
        print(f"â†³ Traceback(ìš”ì•½):\n{tb}")
        return False


# =========================
# 3) OpenAI ìš”ì•½/ì„ë² ë”©
# =========================
def summarize_with_file(pdf_path: str) -> str:
    try:
        f = client.files.create(file=open(pdf_path, "rb"), purpose="assistants")
        resp = client.responses.create(
            model=SUMMARIZE_MODEL,
            input=[{
                "role": "user",
                "content": [
                    {"type": "input_text", "text": SUMMARY_PROMPT},
                    {"type": "input_file", "file_id": f.id}
                ]
            }],
            temperature=0.2
        )
        return (resp.output_text or "").strip()
    except Exception as e:
        print(f"âŒ ìš”ì•½ ì‹¤íŒ¨: {e.__class__.__name__}: {e}")
        tb = traceback.format_exc(limit=3)
        print(f"â†³ Traceback(ìš”ì•½):\n{tb}")
        return ""


def embed_text(text: str) -> list:
    if not text:
        return []
    try:
        er = client.embeddings.create(input=[text], model=EMBED_MODEL)
        return er.data[0].embedding
    except Exception as e:
        print(f"âš ï¸ ì„ë² ë”© ì‹¤íŒ¨(ë¬´ì‹œí•˜ê³  ì§„í–‰): {e}")
        return []


# =========================
# 4) HTML íŒŒì‹±
# =========================
def fetch_notice_html(list_id: str, seq: int) -> Optional[str]:
    try:
        params = {
            "list_id": list_id,
            "seq": str(seq),
            "sort": "1",
            "pageIndex": "1",
            "searchCnd": "",
            "searchWrd": "",
            "cate_id": "",
            "viewAuth": "Y",
            "writeAuth": "Y",
            "board_list_num": "10",
            "lpageCount": "12",
            "menuid": "",
        }
        headers = {"User-Agent": "Mozilla/5.0"}
        r = requests.get(BASE_URL, params=params, headers=headers, timeout=10)
        if r.status_code != 200:
            print(f"âŒ HTTP {r.status_code} for seq={seq}")
            return None
        return r.text
    except Exception as e:
        print(f"âŒ ìš”ì²­ ì‹¤íŒ¨ seq={seq}: {e}")
        return None


def parse_notice_fields(html: str, seq: int) -> Optional[dict]:
    soup = BeautifulSoup(html, "html.parser")
    title_el = soup.select_one("div.vw-tibx h4") if soup else None
    title = title_el.get_text(strip=True) if title_el else ""
    if not title:
        return None  # ê²Œì‹œë¬¼ ì—†ìŒ

    spans = soup.select("div.vw-tibx div.zl-bx div.da span")
    department = spans[1].get_text(strip=True) if len(spans) >= 3 else ""
    date_text = spans[2].get_text(strip=True) if len(spans) >= 3 else ""
    dt = parse_date_yyyy_mm_dd(date_text) or datetime.now().strftime("%Y-%m-%d")

    post_number_el = soup.select_one("input[name=seq]")
    post_number = int(post_number_el["value"]) if post_number_el and post_number_el.get("value") else int(seq)

    content_text = soup.get_text("\n", strip=True)
    return {
        "title": title,
        "department": department,
        "posted_date": dt,
        "post_number": post_number,
        "content_text": content_text,
    }


# =========================
# 5) DB ì—…ì„œíŠ¸
# =========================
UPSERT_SQL = """
INSERT INTO notice
    (category, post_number, title, link, summary, embedding_vector, posted_date, department)
VALUES
    (%s, %s, %s, %s, %s, %s, %s, %s) AS new
ON DUPLICATE KEY UPDATE
    title = new.title,
    link = new.link,
    summary = new.summary,
    embedding_vector = new.embedding_vector,
    posted_date = new.posted_date,
    department = new.department
"""

EXISTS_SQL = "SELECT 1 FROM notice WHERE category=%s AND post_number=%s LIMIT 1"


def upsert_notice(row: dict):
    with mysql_conn() as conn:
        cur = conn.cursor()
        cur.execute(
            UPSERT_SQL,
            (
                row["category"],
                row["post_number"],
                row["title"],
                row["link"],
                row.get("summary") or None,
                row.get("embedding_vector") or None,
                row["posted_date"],
                row.get("department") or None,
            ),
        )
        cur.close()


def exists_notice(category: str, post_number: int) -> bool:
    with mysql_conn() as conn:
        cur = conn.cursor()
        cur.execute(EXISTS_SQL, (category, post_number))
        exists = cur.fetchone() is not None
        cur.close()
        return exists


# =========================
# 6) íŒŒì´í”„ë¼ì¸: í•œ ê±´ ì²˜ë¦¬
#    ë°˜í™˜ê°’: "stored" | "not_found" | "skipped_error"
# =========================
def process_one(category_key: str, list_id: str, seq: int) -> str:
    # 1) HTML
    html = fetch_notice_html(list_id, seq)
    if not html:
        print(f"âš ï¸ Seq {seq}: HTML ë¡œë“œ ì‹¤íŒ¨ â†’ ìŠ¤í‚µ")
        return "skipped_error"

    # 2) íŒŒì‹±
    parsed = parse_notice_fields(html, seq)
    if not parsed:
        print(f"Seq {seq}: ê²Œì‹œë¬¼ ì—†ìŒ")
        return "not_found"

    post_number = parsed["post_number"]
    title = parsed["title"]
    department = parsed["department"]
    posted_date = parsed["posted_date"]

    # 3) ë§í¬
    link = f"{BASE_URL}?list_id={list_id}&seq={seq}"

    # 4) ì¤‘ë³µ ì²´í¬
    if exists_notice(category_key, post_number):
        print(f"Seq {seq} (post_number={post_number}) ì´ë¯¸ ì¡´ì¬ â†’ ìŠ¤í‚µ")
        return "stored"

    # 5) PDF ìƒì„± (ì„±ê³µí•´ì•¼ë§Œ ë‹¤ìŒ ë‹¨ê³„)
    out_pdf = os.path.join(OUT_DIR, f"{category_key}_{seq}.pdf")
    ok_pdf = render_pdf_playwright(link, out_pdf)
    if not ok_pdf:
        print(f"â†³ Seq {seq}: PDF ìƒì„± ì‹¤íŒ¨ë¡œ ì €ì¥ ê±´ë„ˆëœ€ â†’ ë‹¤ìŒ seq")
        return "skipped_error"

    # 6) ìš”ì•½ (PDF ì…ë ¥ë§Œ í—ˆìš©)
    summary = summarize_with_file(out_pdf)
    if not summary:
        print(f"â†³ Seq {seq}: ìš”ì•½ ì‹¤íŒ¨ë¡œ ì €ì¥ ê±´ë„ˆëœ€ â†’ ë‹¤ìŒ seq")
        return "skipped_error"

    # 7) ì„ë² ë”© (ì‹¤íŒ¨í•´ë„ ì €ì¥ì€ ì§„í–‰)
    embedding = embed_text(summary)
    embedding_str = json.dumps(embedding) if embedding else None

    # 8) DB ì—…ì„œíŠ¸
    row = {
        "category": category_key,
        "post_number": post_number,
        "title": title,
        "link": link,
        "summary": summary,
        "embedding_vector": embedding_str,
        "posted_date": posted_date,
        "department": department,
    }
    try:
        upsert_notice(row)
        print(f"âœ… ì €ì¥ ì™„ë£Œ: [{category_key}] seq={seq}, post_number={post_number}, title={title[:30]}...")
        return "stored"
    except MySQLError as e:
        print(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {e.__class__.__name__}({getattr(e,'errno',None)}): {e}")
        tb = traceback.format_exc(limit=3)
        print(f"â†³ Traceback(ìš”ì•½):\n{tb}")
        return "skipped_error"


# =========================
# 7) ì¹´í…Œê³ ë¦¬ í¬ë¡¤ 
#   - ê²Œì‹œë¬¼ 'ì—†ìŒ'ë§Œ miss_countë¡œ ê³„ì‚°í•˜ì—¬ ì¤‘ë‹¨
#   - PDF/ìš”ì•½ ì‹¤íŒ¨ëŠ” ë‹¨ìˆœ ìŠ¤í‚µ í›„ ë‹¤ìŒ seq ì§„í–‰
# =========================
def crawl_category(category_key: str, list_id: str, start_seq: int, missing_break: int = MISSING_BREAK):
    seq = start_seq
    miss_count = 0

    while True:
        status = process_one(category_key, list_id, seq)

        if status == "stored":
            miss_count = 0
        elif status == "not_found":
            miss_count += 1
            print(f"Seq {seq}: ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¯¸ì¡´ì¬ ì¹´ìš´íŠ¸ ëˆ„ì  :  {miss_count}/{missing_break}")
        else:
            # "skipped_error": ì‹¤íŒ¨ ì‚¬ìœ ëŠ” process_oneì—ì„œ ì´ë¯¸ ì¶œë ¥
            pass

        if miss_count >= missing_break:
            print(f"[{category_key}] ì—°ì† {missing_break}ê±´ ì¡´ì¬ X â†’ ì¤‘ë‹¨")
            break

        seq += 1
        time.sleep(REQUEST_SLEEP)


# =========================
# 8) ì‹¤í–‰ ì˜ˆì‹œ
# =========================
if __name__ == "__main__":
    starts = {
        "COLLEGE_ENGINEERING": 15410,
        # í•„ìš”ì‹œ ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ ì¶”ê°€
    }

    for cat, start_seq in starts.items():
        list_id = CATEGORIES.get(cat)
        if not list_id or "TODO" in list_id.lower():
            print(f"â­ï¸  {cat}: list_id ë¯¸ì„¤ì • â†’ ê±´ë„ˆëœ€")
            continue
        print(f"==== [{cat}] list_id={list_id}, start={start_seq} ====")
        crawl_category(cat, list_id, start_seq)
